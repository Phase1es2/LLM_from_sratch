{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-13T22:02:34.977088Z",
     "start_time": "2026-02-13T22:02:34.972444Z"
    }
   },
   "source": [
    "import einx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange, einsum, reduce\n",
    "\n",
    "\n",
    "x: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.ones(2, 3, 4) # inspect x\n",
    "y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.ones(2, 3, 4) # inspect y\n",
    "print(x)\n",
    "print(y)\n",
    "## Basic implementation\n",
    "z = x @ y.transpose(-2, -1)\n",
    "print(z)\n",
    "## Hard to tell the input and output shapes and what they mean.\n",
    "## What shapes can x and y have, and do any of these have unexpected behavior?\n",
    "\n",
    "x: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.ones(2, 3, 4) # inspect x\n",
    "y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.ones(2, 3, 4) # inspect y\n",
    "\n",
    "# Einsum is self-documenting and robust\n",
    "#                       x                   y        ->         z\n",
    "z = einsum(x, y, \"batch seq1 hidden, batch seq2 hidden -> batch seq1 seq2\")\n",
    "print(z)\n",
    "\n",
    "## Or, a batched version where D can have any leading dimensions but AS is constrained.\n",
    "z = einsum(x, y, \"... seq1 hidden, ... seq2 hidden -> ... seq1 seq2\")\n",
    "print(z)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[[4., 4., 4.],\n",
      "         [4., 4., 4.],\n",
      "         [4., 4., 4.]],\n",
      "\n",
      "        [[4., 4., 4.],\n",
      "         [4., 4., 4.],\n",
      "         [4., 4., 4.]]])\n",
      "tensor([[[4., 4., 4.],\n",
      "         [4., 4., 4.],\n",
      "         [4., 4., 4.]],\n",
      "\n",
      "        [[4., 4., 4.],\n",
      "         [4., 4., 4.],\n",
      "         [4., 4., 4.]]])\n",
      "tensor([[[4., 4., 4.],\n",
      "         [4., 4., 4.],\n",
      "         [4., 4., 4.]],\n",
      "\n",
      "        [[4., 4., 4.],\n",
      "         [4., 4., 4.],\n",
      "         [4., 4., 4.]]])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c8cd57ea29f5d20b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T22:57:08.540795Z",
     "start_time": "2026-02-13T22:57:08.485860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images = torch.randn(64, 128, 128, 3) # (batch, height, width, channel)\n",
    "                                                                # (64, 128, 128, 3) (b, H , W, C)\n",
    "dim_by = torch.linspace(start=0.0, end=1.0, steps=10)\n",
    "\n",
    "## Reshape and multiply\n",
    "dim_value = rearrange(dim_by, \"dim_value           -> 1 dim_value 1 1 1\") # (1, 10, 1, 1)\n",
    "images_rearr = rearrange(images, \"b height width channel -> b 1 height width channel\") # resize to (64, 1, 128, 128, 3) => (B, 1, H, W)\n",
    "dimmed_images = images_rearr * dim_value        # (64, 10, 128, 128, 3)\n",
    "\n",
    "## or in one go\n",
    "\"\"\"\n",
    "dimmed_images = einsum(images, dim_by, \"batch height width channel, dim_value -> batch dim_value height width channel\")\n",
    "\"\"\""
   ],
   "id": "548d9afdc9d4ae4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndimmed_images = einsum(images, dim_by, \"batch height width channel, dim_value -> batch dim_value height width channel\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T23:06:01.227820Z",
     "start_time": "2026-02-13T23:06:01.194202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "channels_last = torch.randn(64, 32, 32, 3) #(B, H, W, C)\n",
    "B = torch.randn(32 * 32, 32 * 32)\n",
    "#old way\n",
    "## rearrange an image tensor for mixing across all pixels\n",
    "channels_last_flat = channels_last.view(\n",
    "    -1, channels_last.size(1) * channels_last.size(2), channels_last.size(3)\n",
    ")\n",
    "channels_first_flat = channels_last_flat.transpose(1, 2)\n",
    "channels_first_flat_transformed = channels_first_flat @ B.T\n",
    "channels_last_flat_transformed = channels_first_flat_transformed.transpose(1, 2)\n",
    "channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)\n",
    "\n",
    "\n"
   ],
   "id": "9d1c685500242f1a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T23:11:55.706326Z",
     "start_time": "2026-02-13T23:11:55.677555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "channels_last = torch.randn(64, 32, 32, 3) #(B, H, W, C)\n",
    "B = torch.randn(32 * 32, 32 * 32)\n",
    "height = width = 32\n",
    "## Rearrange replaces clunky torch view + transpose\n",
    "channels_first = rearrange(\n",
    "    channels_last,\n",
    "    \"batch height width channel -> batch channel (height width)\"\n",
    ")\n",
    "channels_first_transformed = einsum(\n",
    "    channels_first, B,\n",
    "    \"batch channel pixel_in, pixel_out pixel_in -> batch channel pixel_out\"\n",
    ")\n",
    "channels_last_transformed = rearrange(\n",
    "    channels_first_transformed,\n",
    "    \"batch channel (height width) -> batch height width channel\",\n",
    "    height=height, width=width\n",
    ")"
   ],
   "id": "3baea824190b5297",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "height = width = 32\n",
    "channels_last_transformed = einx.dot(\n",
    "    \"batch row_in col_in channel, (row_out col_out) (row_in col_in)\"\n",
    "    \"-> batch row_out col_out channel\",\n",
    "    channels_last, B,\n",
    "    col_in=width, col_out=width\n",
    ")"
   ],
   "id": "63941c37f282dc97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
